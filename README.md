# ðŸ§  Streamlit App for Ollama LLMs

A simple and clean Streamlit-based UI for interacting with locally running Ollama LLM models.  
This app makes it easy to send prompts, view responses, and experiment with your installed models through a friendly web interface.

---

## ðŸš€ Features

- Minimal and intuitive Streamlit UI  
- Interact with local Ollama models 
- Dynamic text generation output  
- Easy to extend and customize  

---

## ðŸ“¦ Prerequisites

Before running the application, ensure you have:

1. **Ollama** installed & running locally
   ðŸ‘‰ https://ollama.com/download
2. **Python 3.10+**

---

## ðŸ§  Installation

### Clone the repository:

```bash
git clone https://github.com/Aliyan-12/ollama_streamlit_app.git
cd ollama_streamlit_app
```

### Create and activate a virtual environment:

```bash
python -m venv venv
```
#### Windows
```bash
venv\Scripts\activate
```
#### macOS / Linux
```bash
source venv/bin/activate
```

### Install dependencies:
```bash
pip install -r requirements.txt
```

### Run the application:
```bash
streamlit run app.py
```

